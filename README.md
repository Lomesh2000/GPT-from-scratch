# GPT-from-scratch
# ğŸ§  Mini-GPT â€“ Character-Level Transformer on Shakespeare Data

A lightweight GPT-style decoder-only transformer trained from scratch on Shakespeare text.  
Implements: tokenization, transformer blocks, self-attention, positional embeddings, training loop, and text generation â€” all in pure PyTorch.

---

## ğŸš€ Features
- âœ“ Custom character-level tokenizer (saved + reloadable)
- âœ“ Decoder-only Transformer architecture (GPT-mini)
- âœ“ Trained on Shakespeare dataset
- âœ“ Generate text from a starting prompt
- âœ“ Model + tokenizer saving & loading
- âœ“ TensorBoard metrics logging

---

## ğŸ—ï¸ Model Architecture
| Component | Details |
|----------|---------|
| Type | Decoder-only Transformer |
| Layers | N attention blocks (configurable) |
| Embedding Dim | 384 |
| Attention Heads | multi-head |
| Optimizer | AdamW |
| Loss | Cross-Entropy |

## Hyperparamters
```python
batch_size    = 64
block_size    = 256
max_iters     = 5000
eval_interval = 500
learning_rate = 3e-4
eval_iters    = 200
n_embd        = 384
n_head        = 6
n_layer       = 6
dropout       = 0.2
device        = 'cuda' if torch.cuda.is_available() else 'cpu'
random_seed   = 1337
```

---

## ğŸ“Š Training Metrics

### ğŸ” Training Loss Curve
![Training Loss](losses_curves/train_loss.png)

### ğŸ“‰ Validation Loss Curve
![Validation Loss](losses_curves/val_loss.png)

### ğŸªœ Per-Step Training Loss (Batch-level change)
![Per Step Loss](losses_curves/train_step_loss.png)

> Loss steadily decreases â†’ model is learning patterns from text.

---

## ğŸ§ª Text Generation Example

